{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_features = 16\n",
    "input_window = 30\n",
    "output_window = 5\n",
    "output_features = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKIP CONNECTIONS ARE MISSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_features, output_features, output_window, drop_p):\n",
    "        super(TCN, self).__init__()\n",
    "        self.output_features = output_features\n",
    "        self.output_window = output_window\n",
    "\n",
    "        self.conv1d_1 = nn.Conv1d(in_channels=input_features, out_channels=32, kernel_size=3, groups=input_features)\n",
    "        self.bn1d_1 = nn.BatchNorm1d(32)\n",
    "        self.drop_1 = nn.Dropout1d(p=drop_p)\n",
    "        self.conv1d_2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, groups=32)\n",
    "        self.bn1d_2 = nn.BatchNorm1d(64)\n",
    "        self.drop_2 = nn.Dropout1d(p=drop_p)\n",
    "        self.conv1d_3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, groups=64)\n",
    "        self.bn1d_3 = nn.BatchNorm1d(128)\n",
    "        self.drop_3 = nn.Dropout1d(p=drop_p)\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3))\n",
    "        self.bn2d_1 = nn.BatchNorm2d(32)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3))\n",
    "        self.bn2d_2 = nn.BatchNorm2d(64)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3))\n",
    "        self.bn2d_3 = nn.BatchNorm2d(128)\n",
    "        self.conv2d_4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1,2), dilation=(1,2))\n",
    "        self.bn2d_4 = nn.BatchNorm2d(64)\n",
    "        self.conv2d_5 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(3, 3), stride=(1,2), dilation=(1,2))\n",
    "        self.bn2d_5 = nn.BatchNorm2d(32)\n",
    "        self.conv2d_6 = nn.Conv2d(in_channels=32, out_channels=8, kernel_size=(3, 3), stride=(1,2), dilation=(1,2))\n",
    "        self.bn2d_6 = nn.BatchNorm2d(8)\n",
    "        self.pool_6 = nn.AvgPool2d(kernel_size=(1,2))\n",
    "\n",
    "        self.conv2d_7 = None\n",
    "        # this is the initialization we want to make, but we are making this in the first forward loop to make it dynamic\n",
    "        # self.conv2d_7 = nn.Conv2d(in_channels=8, out_channels=1, kernel_size=(x.shape[2]-output_features+1, x.shape[3]-output_window+1))\n",
    "\n",
    "        self.pad = nn.ReplicationPad2d((1,1,0,0))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        x = self.conv1d_1(x)\n",
    "        x = self.bn1d_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop_1(x)\n",
    "\n",
    "        x = self.pad(x)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.bn1d_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop_2(x)\n",
    "\n",
    "        x = self.conv1d_3(x)\n",
    "        x = self.bn1d_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop_3(x)\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        # x = torch.permute(x, [0,1,3,2])\n",
    "        x = x.permute([0,1,3,2])\n",
    "\n",
    "        x = self.conv2d_1(x)\n",
    "        x = self.bn2d_1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.bn2d_2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.bn2d_3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.bn2d_4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2d_5(x)\n",
    "        x = self.bn2d_5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2d_6(x)\n",
    "        x = self.bn2d_6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool_6(x)\n",
    "\n",
    "        if self.conv2d_7:\n",
    "            x = self.conv2d_7(x)\n",
    "        else:\n",
    "            self.conv2d_7 = nn.Conv2d(in_channels=8, out_channels=1, kernel_size=(x.shape[2]-self.output_features+1, x.shape[3]-self.output_window+1)).to(device)\n",
    "            x = self.conv2d_7(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "    def call(self, x, y):\n",
    "        return self(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189240\n",
      "torch.Size([32, 16, 30])\n",
      "torch.Size([32, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "model = TCN(input_features=16, output_features=4, output_window=5, drop_p=0.2).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "x = torch.randn(batch_size, input_features, input_window).to(device)\n",
    "y = torch.randn(batch_size, output_features, output_window).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.call(x, y)\n",
    "\n",
    "print(x.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=3, drop_p=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=drop_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers=2, drop_p=0.2, teacher_forcing_ratio=1.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=output_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=drop_p)\n",
    "        self.fc = nn.Linear(hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, hidden, cell, output_window, target=None):\n",
    "        batch_size = hidden.size(1)\n",
    "\n",
    "        # Initial decoder input: zeros\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.output_size, device=hidden.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(output_window):\n",
    "            out, (hidden, cell) = self.lstm(decoder_input, (hidden, cell))\n",
    "            pred = self.fc(out.squeeze(1))  # [batch, output_size]\n",
    "            outputs.append(pred.unsqueeze(1))  # [batch, 1, output_size]\n",
    "\n",
    "            if target is not None and torch.rand(1) < self.teacher_forcing_ratio:\n",
    "                decoder_input = target[:, t].unsqueeze(1)  # [batch, 1, output_size]\n",
    "            else:\n",
    "                decoder_input = pred.unsqueeze(1)\n",
    "        \n",
    "        return torch.cat(outputs, dim=1)  # [batch, output_window, output_size]\n",
    "\n",
    "class EncoderDecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, output_window, hidden_size=128, num_layers=3, drop_p=0.2, teacher_forcing_ratio=1.0):\n",
    "        super(EncoderDecoderLSTM, self).__init__()\n",
    "        self.encoder = Encoder(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, drop_p=drop_p)\n",
    "        self.decoder = Decoder(output_size=output_size, hidden_size=hidden_size, num_layers=num_layers, drop_p=drop_p, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        self.output_window = output_window\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        if target is not None:\n",
    "            target = target.permute(0, 2, 1)\n",
    "        \n",
    "        hidden, cell = self.encoder(x)\n",
    "        out = self.decoder(hidden, cell, self.output_window, target)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        return out\n",
    "    \n",
    "    def set_teacher_forcing_ratio(self, new_value):\n",
    "        self.decoder.teacher_forcing_ratio = new_value\n",
    "\n",
    "    def call(self, x, y):\n",
    "        return self(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672260\n",
      "torch.Size([32, 16, 30])\n",
      "torch.Size([32, 4, 5])\n",
      "torch.Size([32, 4, 5])\n",
      "672260\n",
      "torch.Size([32, 16, 30])\n",
      "torch.Size([32, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderLSTM(input_size=input_features, output_size=output_features, output_window=output_window, num_layers=3, drop_p=0.2).to(device)\n",
    "model.train()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "x = torch.randn(batch_size, input_features, input_window).to(device)\n",
    "y = torch.randn(batch_size, output_features, output_window).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.call(x, y)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(output.shape)\n",
    "\n",
    "model = EncoderDecoderLSTM(input_size=input_features, output_size=output_features, output_window=output_window, num_layers=3, drop_p=0.2).to(device)\n",
    "model = model.eval()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "x = torch.randn(batch_size, input_features, input_window).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.call(x, None)\n",
    "\n",
    "print(x.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=3, drop_p=0.2):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=drop_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # [batch, seq_len, features]\n",
    "        outputs, hidden = self.gru(x)  # no cell state in GRU\n",
    "        return hidden  # [num_layers, batch, hidden_size]\n",
    "\n",
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers=2, drop_p=0.2, teacher_forcing_ratio=1.0):\n",
    "        super(GRUDecoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "\n",
    "        self.gru = nn.GRU(input_size=output_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=drop_p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, hidden, output_window, target=None):\n",
    "        batch_size = hidden.size(1)\n",
    "        decoder_input = torch.zeros(batch_size, 1, self.output_size, device=hidden.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(output_window):\n",
    "            out, hidden = self.gru(decoder_input, hidden)\n",
    "            pred = self.fc(out.squeeze(1))  # [batch, output_size]\n",
    "            outputs.append(pred.unsqueeze(1))\n",
    "\n",
    "            if target is not None and torch.rand(1).item() < self.teacher_forcing_ratio:\n",
    "                decoder_input = target[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                decoder_input = pred.unsqueeze(1)\n",
    "\n",
    "        return torch.cat(outputs, dim=1)  # [batch, output_window, output_size]\n",
    "\n",
    "class EncoderDecoderGRU(nn.Module):\n",
    "    def __init__(self, input_size, output_size, output_window, hidden_size=128, num_layers=3, drop_p=0.2, teacher_forcing_ratio=1.0):\n",
    "        super(EncoderDecoderGRU, self).__init__()\n",
    "        self.encoder = GRUEncoder(input_size, hidden_size, num_layers=num_layers, drop_p=drop_p)\n",
    "        self.decoder = GRUDecoder(output_size, hidden_size, num_layers=num_layers, drop_p=drop_p, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        self.output_window = output_window\n",
    "\n",
    "    def forward(self, x, target=None):\n",
    "        if target is not None:\n",
    "            target = target.permute(0, 2, 1)  # [batch, output_window, output_size]\n",
    "\n",
    "        hidden = self.encoder(x)\n",
    "        out = self.decoder(hidden, self.output_window, target)\n",
    "        return out.permute(0, 2, 1)  # [batch, output_size, output_window]\n",
    "\n",
    "    def set_teacher_forcing_ratio(self, new_value):\n",
    "        self.decoder.teacher_forcing_ratio = new_value\n",
    "\n",
    "    def call(self, x, y):\n",
    "        return self(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504324\n",
      "torch.Size([32, 16, 30])\n",
      "torch.Size([32, 4, 5])\n",
      "torch.Size([32, 4, 5])\n",
      "504324\n",
      "torch.Size([32, 16, 30])\n",
      "torch.Size([32, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderGRU(input_size=input_features, output_size=output_features, output_window=output_window, num_layers=3, drop_p=0.2).to(device)\n",
    "model.train()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "x = torch.randn(batch_size, input_features, input_window).to(device)\n",
    "y = torch.randn(batch_size, output_features, output_window).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.call(x, y)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(output.shape)\n",
    "\n",
    "model = EncoderDecoderGRU(input_size=input_features, output_size=output_features, output_window=output_window, num_layers=3, drop_p=0.2).to(device)\n",
    "model = model.eval()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "x = torch.randn(batch_size, input_features, input_window).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.call(x, None)\n",
    "\n",
    "print(x.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))  # [d_model/2]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n",
    "        pe = pe.unsqueeze(1)  # [max_len, 1, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(self, input_features, output_features, input_window, output_window, d_model=128, nhead=8, num_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Project input and output features into d_model space\n",
    "        self.input_proj = nn.Linear(input_features, d_model)\n",
    "        self.output_proj = nn.Linear(output_features, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, output_features)\n",
    "\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dim_feedforward=512, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "\n",
    "    def forward(self, src, tgt=None):\n",
    "        \"\"\"\n",
    "        src: [batch, input_features, input_window]\n",
    "        tgt: [batch, output_features, output_window]\n",
    "        \"\"\"\n",
    "        batch_size = src.size(0)\n",
    "        src = src.permute(0, 2, 1)  # [batch, input_window, input_features]\n",
    "        src = self.input_proj(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        if tgt is not None:\n",
    "            # === Training mode ===\n",
    "            tgt = tgt.permute(0, 2, 1)  # [batch, output_window, output_features]\n",
    "            tgt = self.output_proj(tgt) * math.sqrt(self.d_model)\n",
    "            tgt = self.pos_decoder(tgt)\n",
    "            tgt_mask = self.generate_square_subsequent_mask(self.output_window).to(src.device)\n",
    "            out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "            out = self.output_linear(out)\n",
    "            return out.permute(0, 2, 1)  # [batch, output_features, output_window]\n",
    "        else:\n",
    "            # === Inference mode ===\n",
    "            output = torch.zeros(batch_size, self.output_window, self.output_linear.out_features, device=src.device)\n",
    "\n",
    "            for t in range(self.output_window):\n",
    "                decoder_input = output.clone()  # [batch, output_window, output_features]\n",
    "                decoder_input = self.output_proj(decoder_input) * math.sqrt(self.d_model)\n",
    "                decoder_input = self.pos_decoder(decoder_input)\n",
    "\n",
    "                tgt_mask = self.generate_square_subsequent_mask(self.output_window).to(src.device)\n",
    "                out = self.transformer(src, decoder_input, tgt_mask=tgt_mask)\n",
    "                out = self.output_linear(out)\n",
    "                output[:, t] = out[:, t]  # Take the t-th step output\n",
    "\n",
    "            return output.permute(0, 2, 1)  # [batch, output_features, output_window]\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        return self(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1392388\n",
      "torch.Size([32, 16, 30])\n",
      "torch.Size([32, 4, 5])\n",
      "torch.Size([32, 4, 5])\n",
      "1392388\n",
      "torch.Size([32, 16, 30])\n",
      "torch.Size([32, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderTransformer(input_features=input_features, output_features=output_features, input_window=input_window, output_window=output_window).to(device)\n",
    "model.train()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "x = torch.randn(batch_size, input_features, input_window).to(device)\n",
    "y = torch.randn(batch_size, output_features, output_window).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.call(x, y)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(output.shape)\n",
    "\n",
    "model = EncoderDecoderTransformer(input_features=input_features, output_features=output_features, input_window=input_window, output_window=output_window).to(device)\n",
    "model = model.eval()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "x = torch.randn(batch_size, input_features, input_window).to(device)\n",
    "output = model.call(x, None)\n",
    "\n",
    "print(x.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin-wise Cross Attention LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoinWiseCrossAttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_window, output_window, hidden_dim=128, output_features=4, drop_p=0.2, num_layers=1, num_heads=4, target_coin_index=0):\n",
    "        super().__init__()\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_features = output_features\n",
    "        self.num_coins = 4\n",
    "        self.features_per_coin = 4\n",
    "        self.target_coin_index = target_coin_index\n",
    "\n",
    "        # Create one LSTM per coin\n",
    "        self.lstm_blocks = nn.ModuleList([\n",
    "            nn.LSTM(input_size=self.features_per_coin, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=drop_p\n",
    "            ) for _ in range(self.num_coins)\n",
    "        ])\n",
    "\n",
    "        # Attention: Q, K, V are all the LSTM outputs [batch, 4, hidden_dim]\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True, dropout=drop_p)\n",
    "\n",
    "        # Final projection to output\n",
    "        self.fc = nn.Linear(hidden_dim, output_features * output_window)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, input_features=16, input_window]\n",
    "        Returns: [batch_size, output_features, output_window]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Split input into 4 parts: one per coin\n",
    "        x = x.view(batch_size, self.num_coins, self.features_per_coin, self.input_window)\n",
    "        lstm_outputs = []\n",
    "\n",
    "        for i in range(self.num_coins):\n",
    "            coin_input = x[:, i]  # [batch, 4, input_window]\n",
    "            coin_input = coin_input.permute(0, 2, 1)  # [batch, input_window, 4]\n",
    "            _, (h_n, _) = self.lstm_blocks[i](coin_input)  # h_n: [num_layers, batch, hidden_dim]\n",
    "            lstm_outputs.append(h_n[-1])  # Take last layer: [batch, hidden_dim]\n",
    "\n",
    "        # Stack LSTM outputs: [batch, 4, hidden_dim]\n",
    "        lstm_stack = torch.stack(lstm_outputs, dim=1)\n",
    "\n",
    "        # Self-attention across the 4 coin representations\n",
    "        attn_out, _ = self.attention(lstm_stack, lstm_stack, lstm_stack)  # [batch, 4, hidden_dim]\n",
    "\n",
    "        # Pool across coins (e.g., mean pooling)\n",
    "        target_coin = attn_out[:, self.target_coin_index, :]\n",
    "        \n",
    "        # Predict\n",
    "        output = self.fc(target_coin)  # [batch, output_features * output_window]\n",
    "        output = output.view(batch_size, self.output_features, self.output_window)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def call(self, x, y):\n",
    "        return self(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871444\n",
      "torch.Size([32, 16, 30])\n",
      "torch.Size([32, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "model = CoinWiseCrossAttentionLSTM(input_window, output_window, hidden_dim=128, output_features=4, drop_p=0.2, num_layers=2, num_heads=4, target_coin_index=0).to(device)\n",
    "model.train()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "x = torch.randn(batch_size, input_features, input_window).to(device)\n",
    "y = torch.randn(batch_size, output_features, output_window).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.call(x, y)\n",
    "\n",
    "print(x.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-wise Cross Attention LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureWiseCrossAttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_window, output_window, output_features=4, drop_p=0.2, hidden_dim=128, num_layers=1, num_heads=4, target_coin_index=0):\n",
    "        super().__init__()\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "        self.output_features = output_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_features = 16  # Fixed in this design\n",
    "        self.group_size = 4\n",
    "        self.num_groups = self.num_features // self.group_size\n",
    "        self.target_coin_index = target_coin_index\n",
    "\n",
    "        # One LSTM per feature\n",
    "        self.feature_lstms = nn.ModuleList([\n",
    "            nn.LSTM(input_size=1, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=drop_p)\n",
    "            for _ in range(self.num_features)\n",
    "        ])\n",
    "\n",
    "        # One attention module per group of 4 features\n",
    "        self.group_attentions = nn.ModuleList([\n",
    "            nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True, dropout=drop_p)\n",
    "            for _ in range(self.num_groups)\n",
    "        ])\n",
    "\n",
    "        # Final attention to merge 4 group embeddings\n",
    "        self.final_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True, dropout=drop_p)\n",
    "\n",
    "        # Output projection\n",
    "        self.fc = nn.Linear(hidden_dim, output_features * output_window)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, 16, input_window]\n",
    "        Returns: [batch_size, output_features, output_window]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        feature_representations = []\n",
    "\n",
    "        # Process each feature through its own LSTM\n",
    "        for i in range(self.num_features):\n",
    "            feature_input = x[:, i].unsqueeze(-1)  # [batch, input_window, 1]\n",
    "            _, (h_n, _) = self.feature_lstms[i](feature_input)\n",
    "            feature_representations.append(h_n[-1])  # [batch, hidden_dim]\n",
    "\n",
    "        # Stack into [batch, 16, hidden_dim]\n",
    "        features_stack = torch.stack(feature_representations, dim=1)\n",
    "\n",
    "        # Group features into 4 groups, run attention in each group\n",
    "        group_outputs = []\n",
    "        for i in range(self.num_groups):\n",
    "            group = features_stack[:, i * self.group_size:(i + 1) * self.group_size]  # [batch, 4, hidden_dim]\n",
    "            attn_out, _ = self.group_attentions[i](group, group, group)\n",
    "            group_pooled = attn_out.mean(dim=1)  # [batch, hidden_dim]\n",
    "            group_outputs.append(group_pooled)\n",
    "\n",
    "        # Stack group-level outputs: [batch, 4, hidden_dim]\n",
    "        groups_stack = torch.stack(group_outputs, dim=1)\n",
    "\n",
    "        # Final attention to merge 4 groups\n",
    "        final_attn_out, _ = self.final_attention(groups_stack, groups_stack, groups_stack)\n",
    "        final_embedding = final_attn_out[:, self.target_coin_index, :]\n",
    "\n",
    "        # Project to output\n",
    "        output = self.fc(final_embedding)  # [batch, output_features * output_window]\n",
    "        output = output.view(batch_size, self.output_features, self.output_window)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def call(self, x, y):\n",
    "        return self(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3519508\n",
      "torch.Size([32, 16, 30])\n",
      "torch.Size([32, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "model = FeatureWiseCrossAttentionLSTM(input_window, output_window, hidden_dim=128, output_features=4, drop_p=0.2, num_layers=2, num_heads=4).to(device)\n",
    "model.train()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "x = torch.randn(batch_size, input_features, input_window).to(device)\n",
    "y = torch.randn(batch_size, output_features, output_window).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.call(x,y)\n",
    "\n",
    "print(x.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "490-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
